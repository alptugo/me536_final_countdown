{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP9s2TmvVEM9FmpU9FEe2z8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alptugo/me536_final_countdown/blob/main/ME536_Final_Countdown_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mounting the drive"
      ],
      "metadata": {
        "id": "dFI-2WPwJA9d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-Pl2J5AJuxF",
        "outputId": "f4cd4aac-fb6f-49d0-918b-fc07c9f82ed0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Randomly separating 3 of the 14 lattice types into \"not learned\" group. Getting label array as well for trainning."
      ],
      "metadata": {
        "id": "lMMduaHsKcMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from random import sample\n",
        "import pandas as pd\n",
        "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score, confusion_matrix\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Conv2DTranspose, UpSampling2D, Dropout\n",
        "from tensorflow.keras.metrics import binary_crossentropy\n",
        "import pickle\n",
        "\n",
        "#Giving the directory of the folder by folder 4 channel images\n",
        "path = \"/content/drive/MyDrive/me536_final_countdown/sep_multi_channel_images/\"\n",
        "lattice_types  = os.listdir(path)\n",
        "\n",
        "#defining label to number mapping dictionary\n",
        "label_map = {label:num for num, label in enumerate(lattice_types)}\n",
        "\n",
        "#getting random lattice types to exclude from trainning\n",
        "excluded_lattice_types=[lattice_types[i] for i in sample(range(14), 3)]\n",
        "\n",
        "print(f\"I excluded {excluded_lattice_types}\")\n",
        "\n",
        "#for the first 200 images of every lattice types, opens them, transforms their\n",
        "#channels to binary (from 0 or 255 to 0 or 1). Gets the images and labels to different\n",
        "#lists depending on wether they are excluded or not\n",
        "sequences, excluded_sequences, labels, excluded_labels = [], [], [], []\n",
        "for lattice_type in lattice_types:\n",
        "    for sequence in range(200):\n",
        "      img = Image.open(\"{}{}/{} ({}).png\".format(path, lattice_type, lattice_type, sequence+1))\n",
        "      img_array = np.array(img)\n",
        "      img_array = img_array / 255.0\n",
        "      if lattice_type not in excluded_lattice_types:\n",
        "          sequences.append(img_array)\n",
        "          labels.append(label_map[lattice_type])\n",
        "      else:\n",
        "          excluded_sequences.append(img_array)\n",
        "          excluded_labels.append(label_map[lattice_type])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H55ktF87J0p_",
        "outputId": "282408e2-afdb-4d47-f3e7-525ad5fdf75a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I excluded ['Truncated octahedron', 'Truncated cube', 'Simple cubic']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_map"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbfEzSJ2LCX4",
        "outputId": "e9a43e7d-553d-42fa-88d5-a19990bc738e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Body centered cubic': 0,\n",
              " 'Column': 1,\n",
              " 'Diamond': 2,\n",
              " 'Re-entrant': 3,\n",
              " 'Kelvin cell': 4,\n",
              " 'IsoTruss': 5,\n",
              " 'Columns': 6,\n",
              " 'Octet': 7,\n",
              " 'Face centered cubic': 8,\n",
              " 'Fluorite': 9,\n",
              " 'Simple cubic': 10,\n",
              " 'Truncated octahedron': 11,\n",
              " 'Truncated cube': 12,\n",
              " 'Weaire-Phelan': 13}"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating new label_map by jumping excluded lattice types."
      ],
      "metadata": {
        "id": "sXHZ2VgkLG-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_label_map={label:num for num, label in enumerate([lattice_type for lattice_type in lattice_types if lattice_type not in excluded_lattice_types])}\n"
      ],
      "metadata": {
        "id": "wilaO9daUdxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mapping from old label map new one"
      ],
      "metadata": {
        "id": "vOiQywxKLSeg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "old_to_new_label_map={}\n",
        "for key in label_map.keys():\n",
        "  if key in new_label_map.keys():\n",
        "    old_to_new_label_map = {**old_to_new_label_map, label_map[key]: new_label_map[key]}\n",
        "old_to_new_label_map"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r59syPU5RLw6",
        "outputId": "4805e147-54d8-41d2-d262-6f34b306bd08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 13: 10}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Updating the labels with label map"
      ],
      "metadata": {
        "id": "NPqwwQ2ULkIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels=[old_to_new_label_map[label] for label in labels]"
      ],
      "metadata": {
        "id": "VakqplOUVPor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transforming the labels into keras type structure (there is an array consisted of all classes and where the 1 value represents inclusion on that class)."
      ],
      "metadata": {
        "id": "a-LlhIHFLpm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = tf.keras.utils.to_categorical(labels).astype(int)\n",
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofgshV2pPuU7",
        "outputId": "683c7b72-8bdb-4aef-ee74-f2b4e3bcfeb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2200, 11)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transforming the images into a single numpy array. Splitting this array into lattice types for autoencoder trainning.\n",
        "Also splitting this array and corresponding labels into test and train parts for classifier trainning."
      ],
      "metadata": {
        "id": "2LpiLkDJMOst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array(sequences)\n",
        "print(X.shape)\n",
        "\n",
        "X_by_examples=np.array_split(X, 11)\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, shuffle=True, stratify=y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmboLqGsJ6y7",
        "outputId": "669a1127-76e2-4fd3-a331-c05f0179549c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2200, 48, 48, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape, y_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ANSc9uoN2g9",
        "outputId": "8ee4f657-e4b1-402e-e421-7844de2d2680"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1980, 48, 48, 4) (1980, 11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining a CNN model.\n",
        "The convolutional layers extract features from the input images, while the dense layers perform classification based on those features. The max pooling layers and dropout layer are used to reduce the dimensionality and prevent overfitting."
      ],
      "metadata": {
        "id": "UdHHrTKdNg3r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_input = tf.keras.Input(shape=(48, 48, 4), name='image_input')\n",
        "\n",
        "x = image_input\n",
        "x = tf.keras.layers.Conv2D(32, (3, 3), activation='relu')(x)\n",
        "x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
        "x = tf.keras.layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
        "x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
        "x = tf.keras.layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
        "x = tf.keras.layers.Flatten()(x)\n",
        "x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\n",
        "output = tf.keras.layers.Dense(11, activation='softmax')(x)\n",
        "\n",
        "lattice_classifier = tf.keras.Model(inputs=image_input, outputs=output)"
      ],
      "metadata": {
        "id": "zWwtQu72J-6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining an optimizer for the model. Followed by compiling and trainning."
      ],
      "metadata": {
        "id": "bQJ_jrwpOTdA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "opt = tf.keras.optimizers.Adam(learning_rate=0.002, beta_1=0.9, beta_2=0.9999, epsilon=1e-8, amsgrad=False)\n",
        "lattice_classifier.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "lattice_classifier.fit(X_train, y_train, epochs=8, batch_size=32, validation_split=0.1, shuffle=True)\n",
        "\n",
        "\n",
        "test_loss, test_acc = lattice_classifier.evaluate(X_test, y_test)\n",
        "print('Model Summary:', lattice_classifier.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32-pnVR0KD85",
        "outputId": "2c3edd94-16d3-4bf9-803b-e524e04c5331"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/8\n",
            "56/56 [==============================] - 7s 112ms/step - loss: 0.6931 - categorical_accuracy: 0.7677 - val_loss: 0.0911 - val_categorical_accuracy: 0.9848\n",
            "Epoch 2/8\n",
            "56/56 [==============================] - 6s 109ms/step - loss: 0.0592 - categorical_accuracy: 0.9837 - val_loss: 0.0324 - val_categorical_accuracy: 0.9798\n",
            "Epoch 3/8\n",
            "56/56 [==============================] - 6s 109ms/step - loss: 0.0323 - categorical_accuracy: 0.9927 - val_loss: 0.0071 - val_categorical_accuracy: 1.0000\n",
            "Epoch 4/8\n",
            "56/56 [==============================] - 6s 110ms/step - loss: 0.0472 - categorical_accuracy: 0.9860 - val_loss: 0.0051 - val_categorical_accuracy: 1.0000\n",
            "Epoch 5/8\n",
            "56/56 [==============================] - 6s 111ms/step - loss: 0.0193 - categorical_accuracy: 0.9949 - val_loss: 0.0087 - val_categorical_accuracy: 0.9949\n",
            "Epoch 6/8\n",
            "56/56 [==============================] - 6s 110ms/step - loss: 0.0075 - categorical_accuracy: 0.9989 - val_loss: 0.0068 - val_categorical_accuracy: 0.9949\n",
            "Epoch 7/8\n",
            "56/56 [==============================] - 8s 138ms/step - loss: 0.0080 - categorical_accuracy: 0.9978 - val_loss: 0.0184 - val_categorical_accuracy: 0.9899\n",
            "Epoch 8/8\n",
            "56/56 [==============================] - 6s 109ms/step - loss: 0.0213 - categorical_accuracy: 0.9927 - val_loss: 0.0416 - val_categorical_accuracy: 0.9899\n",
            "7/7 [==============================] - 0s 33ms/step - loss: 0.0205 - categorical_accuracy: 0.9955\n",
            "Model: \"model_23\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " image_input (InputLayer)    [(None, 48, 48, 4)]       0         \n",
            "                                                                 \n",
            " conv2d_58 (Conv2D)          (None, 46, 46, 32)        1184      \n",
            "                                                                 \n",
            " max_pooling2d_24 (MaxPoolin  (None, 23, 23, 32)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_59 (Conv2D)          (None, 21, 21, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_25 (MaxPoolin  (None, 10, 10, 64)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_60 (Conv2D)          (None, 8, 8, 64)          36928     \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 64)                262208    \n",
            "                                                                 \n",
            " dropout_34 (Dropout)        (None, 64)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 11)                715       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 319,531\n",
            "Trainable params: 319,531\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model Summary: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking the models perfomence with Confusion Matrix. Some different displays are commented out as they do not fit screen."
      ],
      "metadata": {
        "id": "brCjIvTnOy4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yhat = lattice_classifier.predict(X_test)\n",
        "ytrue = np.argmax(y_test, axis=1).tolist()\n",
        "yhat = np.argmax(yhat, axis=1).tolist()\n",
        "\n",
        "row_labels = [lattice_type for lattice_type in lattice_types if lattice_type not in excluded_lattice_types]\n",
        "column_labels = [lattice_type for lattice_type in lattice_types if lattice_type not in excluded_lattice_types]\n",
        "df = pd.DataFrame(confusion_matrix(ytrue, yhat), columns=column_labels, index=row_labels)\n",
        "\n",
        "# print('Confusion Matrix:\\n', df, '\\n')\n",
        "# print('Accuracy Score:', accuracy_score(ytrue, yhat), '\\n')\n",
        "print('Confusion Matrix:\\n', confusion_matrix(ytrue, yhat), '\\n')\n",
        "# print('Multilabel Confusion Matrix:\\n', multilabel_confusion_matrix(ytrue, yhat))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRcgIxXaKTug",
        "outputId": "93f6d325-9e24-4855-f577-666a664561dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7/7 [==============================] - 0s 30ms/step\n",
            "Confusion Matrix:\n",
            " [[20  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0 19  0  0  0  0  0  0  0  1  0]\n",
            " [ 0  0 20  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0 20  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0 20  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0 20  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0 20  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0 20  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0 20  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0 20  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0 20]] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the first 20 element of each learned lattice type as autoencoder test data. Rest are used for trainning."
      ],
      "metadata": {
        "id": "SlDqw1nlPr16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_by_examples_train, X_by_examples_test=[X_by_example[20:] for X_by_example in X_by_examples], [X_by_example[:20] for X_by_example in X_by_examples]"
      ],
      "metadata": {
        "id": "yKkHd63-KaGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For every type of learned lattice, a new autoencoder is defined and trained. Each autoencoder generated is appended to a list. The same is performed for encoder part as well, for a possible future need.\n",
        "\n",
        "The convolutional layers with kernel size of (3,3) and relu activation function extract features from the input image.\n",
        "\n",
        "The max pooling layers reduce the spatial dimension of the feature maps by taking the maximum value of a defined neighborhood.\n",
        "\n",
        "The Dropout layers are used to prevent overfitting by randomly setting a fraction of input units to 0.\n",
        "\n",
        "The encoded layer represent the lower dimensional representation of the input image.\n",
        "\n",
        "The UpSampling layers increase the spatial dimensions of the feature maps.\n",
        "\n",
        "The final decoded layer reconstructs the input image by using sigmoid activation function."
      ],
      "metadata": {
        "id": "NAYAqivwP4YS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoders, encoders=[], []\n",
        "\n",
        "#for every lattice type that is not excluded\n",
        "for i in range(len(X_by_examples_train)):\n",
        "\n",
        "  # Create the input layer\n",
        "  input_img = Input(shape=(48, 48, 4))\n",
        "\n",
        "  # Encoder layers\n",
        "  x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
        "  x = Dropout(0.1)(x)\n",
        "  x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "  x = Dropout(0.1)(x)\n",
        "  x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "  x = Dropout(0.1)(x)\n",
        "  x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "\n",
        "  encoded = Conv2D(8, (3, 3), activation='relu', padding='same', name=\"encoded\")(x)\n",
        "\n",
        "  # Decoder layers\n",
        "  x = UpSampling2D((2, 2))(encoded)\n",
        "  x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "  x = UpSampling2D((2, 2))(x)\n",
        "  x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
        "  decoded = Conv2D(4, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "\n",
        "\n",
        "  # Creates the autoencoder model\n",
        "  autoencoder = Model(input_img, decoded)\n",
        "  autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "  # Trains the autoencoder on training data\n",
        "  autoencoder.fit(X_by_examples_train[i], X_by_examples_train[i], epochs=10, batch_size=10, validation_split=0.1)\n",
        "  autoencoders.append(autoencoder)\n",
        "\n",
        "  encoder = Model(input_img, encoded)\n",
        "  encoders.append(encoder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ic8oW5OLKZPb",
        "outputId": "a133a8c7-2ada-4327-9f6b-e4935d4c542e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "17/17 [==============================] - 3s 107ms/step - loss: 0.6857 - val_loss: 0.6579\n",
            "Epoch 2/10\n",
            "17/17 [==============================] - 1s 71ms/step - loss: 0.6365 - val_loss: 0.5939\n",
            "Epoch 3/10\n",
            "17/17 [==============================] - 1s 70ms/step - loss: 0.5661 - val_loss: 0.4619\n",
            "Epoch 4/10\n",
            "17/17 [==============================] - 1s 73ms/step - loss: 0.4381 - val_loss: 0.2926\n",
            "Epoch 5/10\n",
            "17/17 [==============================] - 2s 115ms/step - loss: 0.3357 - val_loss: 0.2180\n",
            "Epoch 6/10\n",
            "17/17 [==============================] - 2s 108ms/step - loss: 0.2771 - val_loss: 0.1787\n",
            "Epoch 7/10\n",
            "17/17 [==============================] - 1s 73ms/step - loss: 0.2380 - val_loss: 0.1689\n",
            "Epoch 8/10\n",
            "17/17 [==============================] - 1s 71ms/step - loss: 0.2177 - val_loss: 0.1447\n",
            "Epoch 9/10\n",
            "17/17 [==============================] - 1s 72ms/step - loss: 0.1987 - val_loss: 0.1292\n",
            "Epoch 10/10\n",
            "17/17 [==============================] - 1s 70ms/step - loss: 0.1845 - val_loss: 0.1143\n",
            "Epoch 1/10\n",
            "17/17 [==============================] - 2s 81ms/step - loss: 0.6475 - val_loss: 0.4993\n",
            "Epoch 2/10\n",
            "17/17 [==============================] - 1s 71ms/step - loss: 0.3678 - val_loss: 0.2810\n",
            "Epoch 3/10\n",
            "17/17 [==============================] - 1s 70ms/step - loss: 0.2395 - val_loss: 0.1871\n",
            "Epoch 4/10\n",
            "17/17 [==============================] - 1s 70ms/step - loss: 0.1619 - val_loss: 0.1371\n",
            "Epoch 5/10\n",
            "17/17 [==============================] - 1s 72ms/step - loss: 0.1201 - val_loss: 0.0990\n",
            "Epoch 6/10\n",
            "17/17 [==============================] - 1s 70ms/step - loss: 0.0857 - val_loss: 0.0612\n",
            "Epoch 7/10\n",
            "17/17 [==============================] - 1s 68ms/step - loss: 0.0502 - val_loss: 0.0302\n",
            "Epoch 8/10\n",
            "17/17 [==============================] - 1s 70ms/step - loss: 0.0360 - val_loss: 0.0257\n",
            "Epoch 9/10\n",
            "17/17 [==============================] - 2s 102ms/step - loss: 0.0378 - val_loss: 0.0170\n",
            "Epoch 10/10\n",
            "17/17 [==============================] - 2s 120ms/step - loss: 0.0191 - val_loss: 0.0110\n",
            "Epoch 1/10\n",
            "17/17 [==============================] - 2s 79ms/step - loss: 0.6873 - val_loss: 0.6674\n",
            "Epoch 2/10\n",
            "17/17 [==============================] - 1s 69ms/step - loss: 0.6402 - val_loss: 0.5870\n",
            "Epoch 3/10\n",
            "17/17 [==============================] - 1s 70ms/step - loss: 0.5765 - val_loss: 0.4893\n",
            "Epoch 4/10\n",
            "17/17 [==============================] - 1s 69ms/step - loss: 0.5227 - val_loss: 0.4266\n",
            "Epoch 5/10\n",
            "17/17 [==============================] - 1s 73ms/step - loss: 0.4786 - val_loss: 0.3912\n",
            "Epoch 6/10\n",
            "17/17 [==============================] - 1s 70ms/step - loss: 0.4427 - val_loss: 0.3485\n",
            "Epoch 7/10\n",
            "17/17 [==============================] - 1s 71ms/step - loss: 0.3990 - val_loss: 0.3045\n",
            "Epoch 8/10\n",
            "17/17 [==============================] - 1s 70ms/step - loss: 0.3702 - val_loss: 0.2738\n",
            "Epoch 9/10\n",
            "17/17 [==============================] - 1s 71ms/step - loss: 0.3459 - val_loss: 0.2473\n",
            "Epoch 10/10\n",
            "17/17 [==============================] - 1s 70ms/step - loss: 0.3250 - val_loss: 0.2154\n",
            "Epoch 1/10\n",
            "17/17 [==============================] - 2s 80ms/step - loss: 0.6455 - val_loss: 0.5711\n",
            "Epoch 2/10\n",
            "17/17 [==============================] - 1s 72ms/step - loss: 0.5352 - val_loss: 0.4806\n",
            "Epoch 3/10\n",
            "17/17 [==============================] - 1s 73ms/step - loss: 0.5055 - val_loss: 0.4730\n",
            "Epoch 4/10\n",
            "17/17 [==============================] - 1s 71ms/step - loss: 0.4838 - val_loss: 0.4387\n",
            "Epoch 5/10\n",
            "17/17 [==============================] - 1s 70ms/step - loss: 0.4405 - val_loss: 0.3610\n",
            "Epoch 6/10\n",
            "17/17 [==============================] - 1s 71ms/step - loss: 0.3417 - val_loss: 0.2316\n",
            "Epoch 7/10\n",
            "17/17 [==============================] - 1s 71ms/step - loss: 0.2447 - val_loss: 0.1378\n",
            "Epoch 8/10\n",
            "17/17 [==============================] - 1s 71ms/step - loss: 0.1891 - val_loss: 0.0999\n",
            "Epoch 9/10\n",
            "17/17 [==============================] - 1s 70ms/step - loss: 0.1611 - val_loss: 0.0735\n",
            "Epoch 10/10\n",
            "17/17 [==============================] - 1s 70ms/step - loss: 0.1447 - val_loss: 0.0690\n",
            "Epoch 1/10\n",
            "17/17 [==============================] - 2s 80ms/step - loss: 0.6826 - val_loss: 0.6655\n",
            "Epoch 2/10\n",
            "17/17 [==============================] - 1s 72ms/step - loss: 0.6540 - val_loss: 0.6371\n",
            "Epoch 3/10\n",
            "17/17 [==============================] - 1s 79ms/step - loss: 0.6054 - val_loss: 0.5484\n",
            "Epoch 4/10\n",
            "17/17 [==============================] - 2s 116ms/step - loss: 0.5031 - val_loss: 0.4391\n",
            "Epoch 5/10\n",
            "17/17 [==============================] - 2s 95ms/step - loss: 0.4187 - val_loss: 0.3610\n",
            "Epoch 6/10\n",
            "17/17 [==============================] - 1s 71ms/step - loss: 0.3548 - val_loss: 0.3031\n",
            "Epoch 7/10\n",
            "17/17 [==============================] - 1s 72ms/step - loss: 0.3153 - val_loss: 0.2640\n",
            "Epoch 8/10\n",
            "17/17 [==============================] - 1s 74ms/step - loss: 0.2846 - val_loss: 0.2353\n",
            "Epoch 9/10\n",
            "17/17 [==============================] - 1s 71ms/step - loss: 0.2653 - val_loss: 0.2159\n",
            "Epoch 10/10\n",
            "17/17 [==============================] - 1s 70ms/step - loss: 0.2492 - val_loss: 0.2066\n",
            "Epoch 1/10\n",
            "17/17 [==============================] - 2s 80ms/step - loss: 0.6809 - val_loss: 0.6568\n",
            "Epoch 2/10\n",
            "17/17 [==============================] - 1s 71ms/step - loss: 0.6431 - val_loss: 0.6321\n",
            "Epoch 3/10\n",
            "17/17 [==============================] - 1s 70ms/step - loss: 0.6186 - val_loss: 0.5969\n",
            "Epoch 4/10\n",
            "17/17 [==============================] - 1s 70ms/step - loss: 0.5767 - val_loss: 0.5365\n",
            "Epoch 5/10\n",
            "17/17 [==============================] - 1s 71ms/step - loss: 0.5166 - val_loss: 0.4668\n",
            "Epoch 6/10\n",
            "17/17 [==============================] - 1s 72ms/step - loss: 0.4682 - val_loss: 0.4199\n",
            "Epoch 7/10\n",
            "17/17 [==============================] - 1s 71ms/step - loss: 0.4302 - val_loss: 0.3773\n",
            "Epoch 8/10\n",
            "17/17 [==============================] - 1s 71ms/step - loss: 0.3975 - val_loss: 0.3365\n",
            "Epoch 9/10\n",
            "17/17 [==============================] - 1s 72ms/step - loss: 0.3698 - val_loss: 0.3042\n",
            "Epoch 10/10\n",
            "17/17 [==============================] - 1s 71ms/step - loss: 0.3451 - val_loss: 0.2742\n",
            "Epoch 1/10\n",
            "17/17 [==============================] - 2s 81ms/step - loss: 0.5424 - val_loss: 0.4293\n",
            "Epoch 2/10\n",
            "17/17 [==============================] - 1s 70ms/step - loss: 0.3918 - val_loss: 0.3283\n",
            "Epoch 3/10\n",
            "17/17 [==============================] - 1s 72ms/step - loss: 0.2825 - val_loss: 0.2506\n",
            "Epoch 4/10\n",
            "17/17 [==============================] - 1s 74ms/step - loss: 0.2175 - val_loss: 0.1915\n",
            "Epoch 5/10\n",
            "17/17 [==============================] - 1s 70ms/step - loss: 0.1392 - val_loss: 0.0973\n",
            "Epoch 6/10\n",
            "17/17 [==============================] - 1s 71ms/step - loss: 0.0489 - val_loss: 0.0614\n",
            "Epoch 7/10\n",
            "17/17 [==============================] - 1s 69ms/step - loss: 0.0198 - val_loss: 0.0543\n",
            "Epoch 8/10\n",
            "17/17 [==============================] - 1s 71ms/step - loss: 0.0121 - val_loss: 0.0699\n",
            "Epoch 9/10\n",
            "17/17 [==============================] - 1s 71ms/step - loss: 0.0086 - val_loss: 0.0539\n",
            "Epoch 10/10\n",
            "17/17 [==============================] - 1s 71ms/step - loss: 0.0066 - val_loss: 0.0446\n",
            "Epoch 1/10\n",
            "17/17 [==============================] - 2s 81ms/step - loss: 0.5788 - val_loss: 0.5253\n",
            "Epoch 2/10\n",
            "17/17 [==============================] - 1s 72ms/step - loss: 0.5176 - val_loss: 0.5133\n",
            "Epoch 3/10\n",
            "17/17 [==============================] - 1s 71ms/step - loss: 0.5073 - val_loss: 0.5009\n",
            "Epoch 4/10\n",
            "17/17 [==============================] - 1s 72ms/step - loss: 0.4920 - val_loss: 0.4722\n",
            "Epoch 5/10\n",
            "17/17 [==============================] - 1s 73ms/step - loss: 0.4567 - val_loss: 0.4190\n",
            "Epoch 6/10\n",
            "17/17 [==============================] - 1s 74ms/step - loss: 0.4131 - val_loss: 0.3607\n",
            "Epoch 7/10\n",
            "17/17 [==============================] - 1s 72ms/step - loss: 0.3433 - val_loss: 0.2497\n",
            "Epoch 8/10\n",
            "17/17 [==============================] - 1s 73ms/step - loss: 0.2673 - val_loss: 0.1924\n",
            "Epoch 9/10\n",
            "17/17 [==============================] - 1s 72ms/step - loss: 0.2325 - val_loss: 0.1662\n",
            "Epoch 10/10\n",
            "17/17 [==============================] - 1s 73ms/step - loss: 0.2138 - val_loss: 0.1513\n",
            "Epoch 1/10\n",
            "17/17 [==============================] - 2s 83ms/step - loss: 0.6852 - val_loss: 0.6460\n",
            "Epoch 2/10\n",
            "17/17 [==============================] - 2s 91ms/step - loss: 0.6210 - val_loss: 0.5747\n",
            "Epoch 3/10\n",
            "17/17 [==============================] - 2s 119ms/step - loss: 0.5247 - val_loss: 0.4255\n",
            "Epoch 4/10\n",
            "17/17 [==============================] - 1s 84ms/step - loss: 0.3646 - val_loss: 0.2725\n",
            "Epoch 5/10\n",
            "17/17 [==============================] - 1s 71ms/step - loss: 0.2607 - val_loss: 0.1932\n",
            "Epoch 6/10\n",
            "17/17 [==============================] - 1s 72ms/step - loss: 0.2109 - val_loss: 0.1600\n",
            "Epoch 7/10\n",
            "17/17 [==============================] - 1s 71ms/step - loss: 0.1887 - val_loss: 0.1500\n",
            "Epoch 8/10\n",
            "17/17 [==============================] - 1s 71ms/step - loss: 0.1773 - val_loss: 0.1452\n",
            "Epoch 9/10\n",
            "17/17 [==============================] - 1s 71ms/step - loss: 0.1686 - val_loss: 0.1296\n",
            "Epoch 10/10\n",
            "17/17 [==============================] - 1s 71ms/step - loss: 0.1615 - val_loss: 0.1273\n",
            "Epoch 1/10\n",
            "17/17 [==============================] - 2s 80ms/step - loss: 0.6252 - val_loss: 0.5534\n",
            "Epoch 2/10\n",
            "17/17 [==============================] - 1s 72ms/step - loss: 0.5539 - val_loss: 0.5290\n",
            "Epoch 3/10\n",
            "17/17 [==============================] - 1s 72ms/step - loss: 0.5387 - val_loss: 0.5083\n",
            "Epoch 4/10\n",
            "17/17 [==============================] - 1s 71ms/step - loss: 0.5073 - val_loss: 0.4254\n",
            "Epoch 5/10\n",
            "17/17 [==============================] - 1s 71ms/step - loss: 0.4462 - val_loss: 0.3700\n",
            "Epoch 6/10\n",
            "17/17 [==============================] - 1s 71ms/step - loss: 0.4077 - val_loss: 0.3154\n",
            "Epoch 7/10\n",
            "17/17 [==============================] - 1s 73ms/step - loss: 0.3741 - val_loss: 0.2870\n",
            "Epoch 8/10\n",
            "17/17 [==============================] - 1s 70ms/step - loss: 0.3365 - val_loss: 0.2354\n",
            "Epoch 9/10\n",
            "17/17 [==============================] - 1s 71ms/step - loss: 0.3020 - val_loss: 0.2026\n",
            "Epoch 10/10\n",
            "17/17 [==============================] - 1s 71ms/step - loss: 0.2675 - val_loss: 0.1705\n",
            "Epoch 1/10\n",
            "17/17 [==============================] - 2s 80ms/step - loss: 0.5218 - val_loss: 0.5238\n",
            "Epoch 2/10\n",
            "17/17 [==============================] - 1s 70ms/step - loss: 0.4714 - val_loss: 0.5103\n",
            "Epoch 3/10\n",
            "17/17 [==============================] - 1s 71ms/step - loss: 0.4553 - val_loss: 0.4978\n",
            "Epoch 4/10\n",
            "17/17 [==============================] - 1s 70ms/step - loss: 0.4323 - val_loss: 0.4648\n",
            "Epoch 5/10\n",
            "17/17 [==============================] - 1s 72ms/step - loss: 0.3962 - val_loss: 0.4222\n",
            "Epoch 6/10\n",
            "17/17 [==============================] - 1s 71ms/step - loss: 0.3506 - val_loss: 0.3757\n",
            "Epoch 7/10\n",
            "17/17 [==============================] - 1s 72ms/step - loss: 0.3085 - val_loss: 0.3334\n",
            "Epoch 8/10\n",
            "17/17 [==============================] - 1s 71ms/step - loss: 0.2787 - val_loss: 0.3101\n",
            "Epoch 9/10\n",
            "17/17 [==============================] - 1s 71ms/step - loss: 0.2579 - val_loss: 0.2876\n",
            "Epoch 10/10\n",
            "17/17 [==============================] - 1s 72ms/step - loss: 0.2412 - val_loss: 0.2665\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each encoder is ran through its trainning data, to get a mean reconstruction error error for each lattice class with its encoder. Also the standard deviation is calcualted as well yet it is not used as simple thresholding was found sufficient."
      ],
      "metadata": {
        "id": "e1feW3EpQ4Hv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "means, stds, rec_errs=[], [], []\n",
        "\n",
        "for autoencoder, image_set in zip(autoencoders, X_by_examples_train):\n",
        "\n",
        "  #reconstructing data\n",
        "  autoencoded_imgs = autoencoder.predict(image_set)\n",
        "\n",
        "  #getting the simple difference mean as well as binary crossentropy error means\n",
        "  mean = np.mean(np.power(image_set - autoencoded_imgs, 2), axis=0)\n",
        "  reconstruction_error = binary_crossentropy(image_set, autoencoded_imgs)\n",
        "  std = np.std(mean)\n",
        "\n",
        "  rec_errs.append(reconstruction_error)\n",
        "  means.append(mean)\n",
        "  stds.append(std)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5o90LoNKb6G",
        "outputId": "82ac4fcf-f767-4907-8270-95e9214498bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6/6 [==============================] - 0s 48ms/step\n",
            "6/6 [==============================] - 0s 48ms/step\n",
            "6/6 [==============================] - 0s 50ms/step\n",
            "6/6 [==============================] - 0s 49ms/step\n",
            "6/6 [==============================] - 0s 46ms/step\n",
            "6/6 [==============================] - 0s 46ms/step\n",
            "6/6 [==============================] - 0s 45ms/step\n",
            "6/6 [==============================] - 0s 44ms/step\n",
            "6/6 [==============================] - 0s 43ms/step\n",
            "6/6 [==============================] - 0s 47ms/step\n",
            "6/6 [==============================] - 0s 49ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting the total of reconstruction error means into a list, so that we would have a single value for every class."
      ],
      "metadata": {
        "id": "hMPmPVUoR1J8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rec_list=[np.mean(rec_errs[i], axis=0).sum() for i in range(11)]\n",
        "rec_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKoucUUJK3P4",
        "outputId": "f1ed9ada-1578-4f52-c2c4-ddc381efd78e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[360.66553,\n",
              " 32.40017,\n",
              " 675.1433,\n",
              " 306.97955,\n",
              " 532.20056,\n",
              " 742.09607,\n",
              " 19.461943,\n",
              " 452.1493,\n",
              " 354.76053,\n",
              " 560.6339,\n",
              " 540.4055]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking if the threshold is valid for test data. If \"is_same=True\", the img selected and the autoencoder is of the same class.The number of inlier must be 20 (and it is). \n",
        "\n",
        "If \"is_same=False\", it checks for different autoencoder and image groups. The number of inliers must be 0 (and it is almost always)."
      ],
      "metadata": {
        "id": "dy_NSWeMSJFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#randomly selecting 2 groups to crosscheck encoders with different images\n",
        "selection_group, image_from_group = sample(range(11), 2)\n",
        "is_same=False\n",
        "\n",
        "if is_same:\n",
        "  selection_group, image_from_group = selection_group, selection_group\n",
        "\n",
        "\n",
        "print(selection_group, image_from_group)\n",
        "\n",
        "\n",
        "in_number, out_number=0,0\n",
        "for in_group_index in range(len(X_by_examples_test[selection_group])):\n",
        "\n",
        "      #gets the image from the test group\n",
        "      new_img=X_by_examples_test[image_from_group][in_group_index]\n",
        "      new_img= np.expand_dims(new_img, axis=0)\n",
        "\n",
        "      #gets the reconstructed image\n",
        "      autoencoded_new_img = autoencoders[selection_group].predict(new_img, verbose=0)\n",
        "\n",
        "      #calculates the reconstruction error sum\n",
        "      reconstruction_error = binary_crossentropy(new_img, autoencoded_new_img)\n",
        "      rec_error=np.sum(reconstruction_error)\n",
        "\n",
        "      #thresholds the reconstruction error with the groups error\n",
        "      if rec_error>rec_list[selection_group]*2.4:\n",
        "        out_number+=1\n",
        "      else:\n",
        "        in_number+=1\n",
        "\n",
        "print(in_number)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3P-cBZYYLBPt",
        "outputId": "88ad3d85-4605-4af9-850a-e20bd3dead33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8 2\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting the inverse of the old and new label map, so that when the model gives a number, we can get the name of the lattice"
      ],
      "metadata": {
        "id": "aYNeobNJS7u0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inv_map = {label_map[k] : k for k in label_map}\n",
        "\n",
        "new_inv_map={new_label_map[k] : k for k in new_label_map}"
      ],
      "metadata": {
        "id": "d82X3jm1axqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function that controls if an image is realy from that class. The functionized and singular form the of the previous block."
      ],
      "metadata": {
        "id": "hVVdmGZcTbai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loyalty_checker(autoencoder_list, img, preverdict, rec_list):\n",
        "  '''This is a function for detecting if a classification is correct or if it\n",
        "  is the overconfidence of the classifier. The inputs are the autoencder list containing\n",
        "  the autoencoder for each class, the image being tested, the classification of the classifier\n",
        "  and the mean reconstruction error sum list conatinaing data for each lattice type.'''\n",
        "\n",
        "  autoencoded_new_img = autoencoder_list[preverdict].predict(img, verbose=0)\n",
        "  reconstruction_error = binary_crossentropy(img, autoencoded_new_img)\n",
        "  rec_error=np.sum(reconstruction_error)\n",
        "\n",
        "  if rec_error>rec_list[preverdict]*2.4:\n",
        "    return False\n",
        "  else:\n",
        "    return True"
      ],
      "metadata": {
        "id": "4K7IW8iBiLdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "excluded_lattice_types"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RL7IU7zTcd4v",
        "outputId": "d5170beb-a101-4e33-8a69-053fe440fe4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Truncated octahedron', 'Truncated cube', 'Simple cubic']"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_inv_map"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_i56RP-fzcL",
        "outputId": "803c5ec3-3c8a-4cca-d6b1-552b4629e1c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'Body centered cubic',\n",
              " 1: 'Column',\n",
              " 2: 'Diamond',\n",
              " 3: 'Re-entrant',\n",
              " 4: 'Kelvin cell',\n",
              " 5: 'IsoTruss',\n",
              " 6: 'Columns',\n",
              " 7: 'Octet',\n",
              " 8: 'Face centered cubic',\n",
              " 9: 'Fluorite',\n",
              " 10: 'Weaire-Phelan'}"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the dictionary that maps zip file names to 4 channels images, so that the input can be given as both .zip file or .png file."
      ],
      "metadata": {
        "id": "rJ_4xBjtToa6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/me536_final_countdown/file_name_mapper.pickle', 'rb') as handle:\n",
        "    zip2img_map = pickle.load(handle)"
      ],
      "metadata": {
        "id": "_jcI-Nb_98o3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##THIS IS THE RESULT PART!\n",
        "\n",
        "On this part, it asks for either a .zip or a 4 channel .png file. If a .zip is given, it searches for corresponding image file, if there is no such image file, that means the given .zip file does not represents a 3D mesh file with a lattice. Therefore it gives the output by stating this type of novelty.\n",
        "\n",
        "If the .zip file corresponds to a 4 channel image, the image is classified. If the lattice is not from the known classes, the novelty is specified by stating that the lattice is from a new class.\n",
        "\n",
        "If a .png type of data is given, it is guaranteed that the data contains lattices, therefore the output is either a classification or the notification of a novel type of lattice."
      ],
      "metadata": {
        "id": "HolF9FCfUm5o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_path=input(\"Please give the path of a 4 channel image or a zip. For 4 channel images,\\\n",
        "any index greater than 200 was not used on either training or testing of models,\\\n",
        "so feel free to use them. Enter 'q' to stop.\\n\")\n",
        "\n",
        "#As I tried transfer learning earlier, I was changing the model I used when \n",
        "#I encountered new data. This things here are relics of that.\n",
        "last_model=lattice_classifier\n",
        "new_lattice_types=[]\n",
        "novelty_counter=0\n",
        "while input_path!=\"q\":\n",
        "\n",
        "  #checks the inputs type\n",
        "  if input_path[-4:]==\".png\":\n",
        "    new_img=Image.open(input_path)\n",
        "  elif input_path[-4:]==\".zip\":\n",
        "    zip_name=input_path.split(\"/\")[-1]\n",
        "\n",
        "    #checks wether the input is transformed to a 4 channel img\n",
        "    if zip_name in zip2img_map.keys():\n",
        "      img_name=zip2img_map[zip_name]\n",
        "      folder_name=zip_name.lstrip('1234567890')[:-4]\n",
        "\n",
        "      #sets the path to the corresponding image file\n",
        "      input_path=f\"/content/drive/MyDrive/me536_final_countdown/sep_multi_channel_images/{folder_name}/{img_name}\"\n",
        "      print(f\"4 channel img found on /content/drive/MyDrive/me536_final_countdown/sep_multi_channel_images/{folder_name}/{img_name}\")\n",
        "      new_img=Image.open(input_path)\n",
        "\n",
        "\n",
        "    else:\n",
        "      print(\"There is no such 4 channel image, your zip doesn't contain lattices...\\n\")\n",
        "      input_path=input(\"Please give the path of a 4 channel image or a zip. For 4 channel images,\\\n",
        "      any index greater than 200 was not used on either training or testing of models,\\\n",
        "      so feel free to use them. Enter 'q' to stop.\\n\")\n",
        "      continue\n",
        "  new_img = np.array(new_img)\n",
        "  new_img=new_img/255\n",
        "  new_img= np.expand_dims(new_img, axis=0)\n",
        "\n",
        "  #gets lattice type as number by predicting through classifier\n",
        "  type_number=int(np.argmax(last_model.predict(new_img)))\n",
        "\n",
        "  #gets lattice type name from label map\n",
        "  type_text=new_inv_map[type_number]\n",
        "  print(f\"Preliminarily it is {type_text},\\\n",
        "  but gotta check if it is an overconfidence against a novelty\")\n",
        "  if type_text not in new_lattice_types:\n",
        "    #checks wether it is realy from that class or a novelty using loyalty_checker\n",
        "    #function\n",
        "    is_inlier=loyalty_checker(autoencoders, new_img, type_number, rec_list)\n",
        "\n",
        "    if is_inlier:\n",
        "      print(\"It really is an inlier\\n\")\n",
        "    else:\n",
        "      print(\"It is not really from this group. We have an alien\\n\")\n",
        "\n",
        "\n",
        "  input_path=input(\"Please give the path of a 4 channel image or a zip. For 4 channel images,\\\n",
        "  any index greater than 200 was not used on either training or testing of models,\\\n",
        "   so feel free to use them. Enter 'q' to stop.\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBhgmcaN86K7",
        "outputId": "1fdeb88f-e0e2-4ada-a885-6667baa7e290"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please give the path of a 4 channel image or a zip. For 4 channel images,any index greater than 200 was not used on either training or testing of models,so feel free to use them. Enter 'q' to stop.\n",
            "/content/drive/MyDrive/me536_final_countdown/images/10Simple cubic.zip\n",
            "There is no such 4 channel image, your zip doesn't contain lattices...\n",
            "\n",
            "Please give the path of a 4 channel image or a zip. For 4 channel images,      any index greater than 200 was not used on either training or testing of models,      so feel free to use them. Enter 'q' to stop.\n",
            "/content/drive/MyDrive/me536_final_countdown/images/20Simple cubic.zip\n",
            "4 channel img found on /content/drive/MyDrive/me536_final_countdown/sep_multi_channel_images/Simple cubic/Simple cubic (59).png\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Preliminarily it is Re-entrant,  but gotta check if it is an overconfidence against a novelty\n",
            "It is not really from this group. We have an alien\n",
            "\n",
            "Please give the path of a 4 channel image or a zip. For 4 channel images,  any index greater than 200 was not used on either training or testing of models,   so feel free to use them. Enter 'q' to stop.\n",
            "/content/drive/MyDrive/me536_final_countdown/images/102Diamond.zip\n",
            "4 channel img found on /content/drive/MyDrive/me536_final_countdown/sep_multi_channel_images/Diamond/Diamond (2).png\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Preliminarily it is Diamond,  but gotta check if it is an overconfidence against a novelty\n",
            "It really is an inlier\n",
            "\n",
            "Please give the path of a 4 channel image or a zip. For 4 channel images,  any index greater than 200 was not used on either training or testing of models,   so feel free to use them. Enter 'q' to stop.\n",
            "/content/drive/MyDrive/me536_final_countdown/images/342Octet.zip\n",
            "4 channel img found on /content/drive/MyDrive/me536_final_countdown/sep_multi_channel_images/Octet/Octet (202).png\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "Preliminarily it is Octet,  but gotta check if it is an overconfidence against a novelty\n",
            "It really is an inlier\n",
            "\n",
            "Please give the path of a 4 channel image or a zip. For 4 channel images,  any index greater than 200 was not used on either training or testing of models,   so feel free to use them. Enter 'q' to stop.\n",
            "q\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below is an attempt on transfer learning, however it only worked correctly once (off many many tries). As the new data set used for adapting is too small (3 images) it doesn't realy function well for now. \n",
        "\n",
        "I simply tried to freeze all layers except the last one. I redefined the last layer for new amount of classes, and fed in new data. Moreover I tried to both use 3 images of every class and only the new class."
      ],
      "metadata": {
        "id": "6m3wrgdRgvwc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inv_map = {label_map[k] : k for k in label_map}\n",
        "\n",
        "X_train_tiny=np.concatenate([latt_type[:3] for latt_type in X_by_examples], axis=0)\n",
        "\n",
        "y_train_tiny=[i for i in range(11) for k in range(3)]\n",
        "\n",
        "y_train_tiny = tf.keras.utils.to_categorical(y_train_tiny).astype(int)\n",
        "\n",
        "input_path=input(\"Please give the path of an image. Any index greater than 200\\\n",
        "was not used on either training or testing of models, so feel free to use them.\\\n",
        "Enter 'q' to stop.\\n\")\n",
        "\n",
        "last_model=lattice_classifier\n",
        "new_lattice_types=[]\n",
        "novelty_counter=0\n",
        "while input_path!=\"q\":\n",
        "  if input_path[:-4]==\".png\":\n",
        "    new_img=Image.open(input_path)\n",
        "  elif input_path[:-4]==\".zip\":\n",
        "    zip_name=input_path.split(\"/\")[-1]\n",
        "    if zip_name in zip2img_map.keys():\n",
        "      img_name=zip2img_map[zip_name]\n",
        "      folder_name=zip_name.lstrip('1234567890')[:-4]\n",
        "      input_path=f\"/content/drive/MyDrive/me536_final_countdown/sep_multi_channel_images/{folder_name}/{img_name}\"\n",
        "      print(f\"4 channel img found on /content/drive/MyDrive/me536_final_countdown/sep_multi_channel_images/{folder_name}/{img_name}\")\n",
        "      new_img=Image.open(input_path)\n",
        "    else:\n",
        "      print(\"There is no such 4 channel image, your zip doesn't contain lattices...\")\n",
        "      continue\n",
        "  new_img = np.array(new_img)\n",
        "  new_img=new_img/255\n",
        "\n",
        "  new_img= np.expand_dims(new_img, axis=0)\n",
        "  type_number=int(np.argmax(last_model.predict(new_img)))\n",
        "  type_text=new_inv_map[type_number]\n",
        "  print(f\"Preliminarily it is {type_text},\\\n",
        "  but gotta check if it is an overconfidence against a novelty\")\n",
        "  if type_text not in new_lattice_types:\n",
        "    is_inlier=loyalty_checker(autoencoders, new_img, type_number, rec_list)\n",
        "\n",
        "    if is_inlier:\n",
        "      print(\"It really is an inlier\")\n",
        "    else:\n",
        "      print(\"It is not really from this group. Could you give 2 more of this, so it can learn?\")\n",
        "      input_path=input()\n",
        "\n",
        "      if input_path[:-4]==\".png\":\n",
        "        new_img=Image.open(input_path)\n",
        "      elif input_path[:-4]==\".zip\":\n",
        "        zip_name=input_path.split(\"/\")[-1]\n",
        "      if zip_name in zip2img_map.keys():\n",
        "        img_name=zip2img_map[zip_name]\n",
        "        folder_name=zip_name.lstrip('1234567890')[:-4]\n",
        "        input_path=f\"/content/drive/MyDrive/me536_final_countdown/sep_multi_channel_images/{folder_name}/{img_name}\"\n",
        "        print(f\"4 channel img found on /content/drive/MyDrive/me536_final_countdown/sep_multi_channel_images/{folder_name}/{img_name}\")\n",
        "      new_img2=Image.open(input_path)\n",
        "\n",
        "      new_img2=Image.open(new_img2)\n",
        "      new_img2 = np.array(new_img2)\n",
        "      new_img2=new_img/255\n",
        "\n",
        "\n",
        "      if input_path[:-4]==\".png\":\n",
        "        new_img=Image.open(input_path)\n",
        "      elif input_path[:-4]==\".zip\":\n",
        "        zip_name=input_path.split(\"/\")[-1]\n",
        "      if zip_name in zip2img_map.keys():\n",
        "        img_name=zip2img_map[zip_name]\n",
        "        folder_name=zip_name.lstrip('1234567890')[:-4]\n",
        "        input_path=f\"/content/drive/MyDrive/me536_final_countdown/sep_multi_channel_images/{folder_name}/{img_name}\"\n",
        "        print(f\"4 channel img found on /content/drive/MyDrive/me536_final_countdown/sep_multi_channel_images/{folder_name}/{img_name}\")\n",
        "      new_img3=Image.open(input_path)\n",
        "\n",
        "      new_img3=input()\n",
        "      new_img3=Image.open(new_img3)\n",
        "      new_img3 = np.array(new_img3)\n",
        "      new_img3=new_img/255\n",
        "\n",
        "\n",
        "      last_layer_of_classifier=last_model.layers[-2]\n",
        "      new_class_count=last_model.layers[-1].output_shape[1]+1\n",
        "      new_dense_layer=tf.keras.layers.Dense(new_class_count, activation='softmax')(last_layer_of_classifier.output)\n",
        "      new_lattice_classifier=Model(inputs=last_model.input, outputs=new_dense_layer)\n",
        "\n",
        "      for layer in lattice_classifier.layers[:-1]:\n",
        "        layer.trainable = False\n",
        "      # X_train_new=np.concatenate((X_train, new_img, new_img2, new_img3), axis=0)\n",
        "\n",
        "      if novelty_counter==0:\n",
        "\n",
        "        # y_train_tiny=np.hstack((y_train_tiny, np.zeros((33,1))))\n",
        "\n",
        "        novelty_label_train=np.zeros((3, new_class_count))\n",
        "        novelty_label_train[:,[-1]]=np.ones((3,1))\n",
        "        novelty_label_train=novelty_label_train.astype(int)\n",
        "        # novelty_label_train=np.concatenate((y_train_tiny, novelty_label_train), axis=0)\n",
        "\n",
        "\n",
        "        # novelty_train=np.concatenate((X_train_tiny, new_img, new_img2, new_img3), axis=0)\n",
        "        novelty_train=np.concatenate((new_img, new_img2, new_img3), axis=0)\n",
        "\n",
        "\n",
        "      else:\n",
        "        new_rows=np.zeros((3, new_class_count))\n",
        "        new_rows[:,[-1]]=np.ones((3,1))\n",
        "        new_rows=new_rows.astype(int)\n",
        "        novelty_label_train=np.hstack((novelty_label_train, np.zeros((novelty_label_train.shape[0],1))))\n",
        "        novelty_label_train=np.concatenate((novelty_label_train, new_rows), axis=0)\n",
        "\n",
        "        novelty_train=np.concatenate((novelty_train, new_img, new_img2, new_img3), axis=0)\n",
        "\n",
        "      opt = tf.keras.optimizers.Adam(learning_rate=0.002, beta_1=0.9, beta_2=0.9999, epsilon=1e-8, amsgrad=False)\n",
        "      new_lattice_classifier.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "      \n",
        "      early_stop = tf.keras.callbacks.EarlyStopping(monitor='categorical_accuracy', min_delta=0.001, patience=5, mode='max', restore_best_weights=True)\n",
        "\n",
        "      new_lattice_classifier.fit(novelty_train, novelty_label_train, epochs=30, batch_size=3, validation_split=0.4, shuffle=True, callbacks=[early_stop])\n",
        "      \n",
        "      new_label_map[f\"NewLattice{novelty_counter}\"]=new_class_count-1\n",
        "      new_inv_map[new_class_count-1]=f\"NewLattice{novelty_counter}\"\n",
        "      new_lattice_types.append(f\"NewLattice{novelty_counter}\")\n",
        "\n",
        "\n",
        "      print(f\"New lattice type called NewLattice{novelty_counter} is defined. You can give it a try!\")\n",
        "      last_model=new_lattice_classifier\n",
        "      novelty_counter+=1\n",
        "  else:\n",
        "    print(\"Well it seems like this type is learned on the fly, so there isn't\\\n",
        "     enough data to crosscheck this classification.\")\n",
        "\n",
        "  input_path=input(\"Please give the path of an image. Any index greater than 200\\\n",
        "  was not used on either training or testing of models, so feel free to use them.\\\n",
        "  Enter 'q' to stop.\\n\")\n"
      ],
      "metadata": {
        "id": "QpZkyL9YCPDw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}